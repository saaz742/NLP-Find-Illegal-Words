{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SKqR_-tbMMw4",
        "ur_jmf4ZQU3z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <strong><font color=#A52A2A></span> NLP_HW2 </strong>\n",
        "Dr Asgari - Spring 2023\n",
        "<br>\n",
        "98101566 -> Mohammadreza Daviran\n",
        "<br>\n",
        "98171007 -> Nona Ghazizadeh\n",
        "<br>\n",
        "98170668 -> Sara Azarnoush\n",
        "<br>\n",
        "\n",
        "\n",
        "<strong><font color=#B22222></span> Please run this notebook on google colab! </strong>"
      ],
      "metadata": {
        "id": "aoPbVdLmGqk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find illegal words in a text\n",
        "\n",
        "- Requirements\n",
        "  - Download libraries\n",
        "  - Download persian fasttext model\n",
        "  - Import libraries\n",
        "- Preprocess the data\n",
        "  - Normalizer and Lemmatizer\n",
        "  - Download abbreviations\n",
        "  - Read abbreviation\n",
        "  - Numbers\n",
        "  - Read persian fasttext model\n",
        "- Process the data\n",
        "  - Find persian words\n",
        "  - Find similar words in context\n",
        "  - Print illegal words\n",
        "  - Find words ignoring spaces\n",
        "  - Print illegal words considering the spaces\n",
        "- Test\n",
        "  - Test without space in each word\n",
        "  - Test with space in each word\n",
        "  - Test with abbreviation in text\n",
        "  - Test with normalizer\n",
        "  - Test with lemmatizer\n",
        "  - Test with context"
      ],
      "metadata": {
        "id": "1-o28CJrM4lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "------\n",
        "\n",
        "Download libraries"
      ],
      "metadata": {
        "id": "okL31sNTL-Vl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QmGLoP_j0D0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1932993d-06d2-471f-9737-7d9489b5670c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=9483f6e37abc1bc7605b21e84565f6f672e378548bd320e1389a0c4a3db874f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/62/f6/88933dadcd64a1614894614aa68cf57c4b8e5256acb650b1f1\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp39-cp39-linux_x86_64.whl size=180375 sha256=95ae50027f770842c750c1afe25a73d9b59967ef8d39e906fa071a634611960d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/ff/82/9326b96f96f47472e02c453697b225813e4650c0ed4df2cd49\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.10.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (67.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395795 sha256=cc628473c69d79ac089cbd1f4988d9668e559b9562a7db4351f239dc31b94c7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.20.9 rapidfuzz-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "!pip install gdown\n",
        "!pip install fasttext\n",
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download persian fasttext model"
      ],
      "metadata": {
        "id": "slyJKnGsTRlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gzip -d cc.fa.300.bin.gz"
      ],
      "metadata": {
        "id": "Qko1MQDwHU7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1d015d-c8be-4651-c4f2-a82283ea51b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-03 11:28:58--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.162.108, 108.157.162.83, 108.157.162.35, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.162.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4502524724 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.fa.300.bin.gz’\n",
            "\n",
            "cc.fa.300.bin.gz    100%[===================>]   4.19G   120MB/s    in 34s     \n",
            "\n",
            "2023-04-03 11:29:33 (125 MB/s) - ‘cc.fa.300.bin.gz’ saved [4502524724/4502524724]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "CysZklwGOWpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import re\n",
        "from hazm import *\n",
        "import json\n",
        "import gdown\n",
        "import fasttext\n",
        "import Levenshtein"
      ],
      "metadata": {
        "id": "Ron_ot5QJ9Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and initialize our requirments\n",
        "------\n",
        "Normalizer and lemmatizer "
      ],
      "metadata": {
        "id": "SKqR_-tbMMw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()"
      ],
      "metadata": {
        "id": "-3_jkKfUDAyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download abbreviations"
      ],
      "metadata": {
        "id": "Yk0WHK83PFes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/uc?id=12QXnsKtysJzs9r0W-ONMSAMnNuxS8hAa\"\n",
        "output = \"abbreviation.json\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "diqTaeDQuu2M",
        "outputId": "4173ee21-a90e-4cde-cc42-4477e30ad433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12QXnsKtysJzs9r0W-ONMSAMnNuxS8hAa\n",
            "To: /content/abbreviation.json\n",
            "100%|██████████| 14.5k/14.5k [00:00<00:00, 15.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abbreviation.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read abbreviation"
      ],
      "metadata": {
        "id": "l8h48x_-Psqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('abbreviation.json', 'r') as file:\n",
        "    abbreviation = json.load(file)"
      ],
      "metadata": {
        "id": "FnsAEOpCuMKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numbers"
      ],
      "metadata": {
        "id": "PQPIMYABQBj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persian_numbers = '۱۲۳۴۵۶۷۸۹۰'\n",
        "english_numbers = '1234567890'\n",
        "english_trans = str.maketrans(persian_numbers, english_numbers)"
      ],
      "metadata": {
        "id": "EqJ_Bs1DQHOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load persian fasttext model"
      ],
      "metadata": {
        "id": "n59ZqWJRP3OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model('cc.fa.300.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE06n-vGa-hG",
        "outputId": "18125f27-3da8-4c33-eafe-b9947b30c03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the data\n",
        "-----\n"
      ],
      "metadata": {
        "id": "ur_jmf4ZQU3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find persian word"
      ],
      "metadata": {
        "id": "_fQNdt3CaCBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_persian(string: str):\n",
        "    return \"\".join(re.findall(r\"[\\u0600-\\u06FF]+\", string))"
      ],
      "metadata": {
        "id": "tIKRW5L-Z7u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find cotextually similar words to given word"
      ],
      "metadata": {
        "id": "GNKrF6yyUpOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_words_in_context(given_word, k_num=30):\n",
        "    similar_words = model.get_nearest_neighbors(given_word, k=k_num)\n",
        "    founded_words = []\n",
        "    for similarity, word in similar_words:\n",
        "        if given_word not in word and Levenshtein.distance(word, given_word) > 3 and similarity > 0.5:\n",
        "            founded_words.append(word)\n",
        "    return founded_words"
      ],
      "metadata": {
        "id": "47hQJF2hbBxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join and print illegal word"
      ],
      "metadata": {
        "id": "WMdFVNNyaLrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_illegal_words(input, ill_words):\n",
        "  if len(ill_words) > 0:\n",
        "    print('Without any space between each word: \\n')\n",
        "  for w in ill_words:\n",
        "    complete_word = ill_words[w]\n",
        "    index = input.find(complete_word)\n",
        "    print('word: \\\"' + w + '\\\"')\n",
        "    print(f'span: ({index}, {index + len(complete_word)})\\n')"
      ],
      "metadata": {
        "id": "SAjJf2lb08L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find and print words without spaces"
      ],
      "metadata": {
        "id": "WqtcBeGmSXX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_words_ignoring_spaces_2(splitted_input, transfered_words):\n",
        "  found_spaced_ill_words = {}\n",
        "  for i in range(len(transfered_words) - 1):\n",
        "    w1 = transfered_words[i]\n",
        "    w2 = transfered_words[i + 1]\n",
        "    word = w1 + w2\n",
        "    if word in illegal_words:\n",
        "      found_spaced_ill_words[word] = input[input.find(splitted_input[i]): input.find(splitted_input[i + 1])] + w2\n",
        "  return found_spaced_ill_words"
      ],
      "metadata": {
        "id": "_igXQtvZ1e43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_words_ignoring_spaces(splitted_input, transfered_words, indices):\n",
        "  found_spaced_ill_words = {}\n",
        "  \n",
        "  text = \"\".join(transfered_words)\n",
        "  for word in illegal_words:\n",
        "    range = []\n",
        "    word = word.strip()\n",
        "    f = text.find(word)\n",
        "    if f > 0:\n",
        "      flag = 0\n",
        "      for counter, index in enumerate(indices):\n",
        "        if index[0] == f:\n",
        "          f = f + len(word)\n",
        "          range.append(counter)\n",
        "          flag = 1\n",
        "        if flag == 1 and index[1] == f:\n",
        "          range.append(counter)\n",
        "          break\n",
        "      if range[0] != range[-1]:\n",
        "        found_spaced_ill_words[word] = input[input.find(splitted_input[range[0]]): input.find(splitted_input[range[-1] + 1])]\n",
        "  return found_spaced_ill_words"
      ],
      "metadata": {
        "id": "aVVnNxG4aS4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_illegal_words_space(found_spaced_ill_words):\n",
        "    if len(found_spaced_ill_words) > 0:\n",
        "        print('\\nWith white space between each word: \\n')\n",
        "    for w in found_spaced_ill_words:\n",
        "        complete_word = found_spaced_ill_words[w]\n",
        "        index = input.find(complete_word)\n",
        "        print('word: \\\"' + w + '\\\"')\n",
        "        print(f'span: ({index}, {index + len(complete_word)})\\n')"
      ],
      "metadata": {
        "id": "scq81XQl21vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function"
      ],
      "metadata": {
        "id": "_9MBjPsONyla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(input: str, illegal_words: list):\n",
        "    status = 1\n",
        "  \n",
        "    splitted_input = input.split()\n",
        "\n",
        "    found_ill_words = {}\n",
        "    transfered_words = []\n",
        "    actual_words = []\n",
        "    indices = []\n",
        "    counter = 0\n",
        "    for word in splitted_input:\n",
        "        modified_word = str(word).translate(english_trans)\n",
        "        transfered_word = find_persian(modified_word)\n",
        "        new_counter = counter + len(transfered_word)\n",
        "        indices.append((counter, new_counter))\n",
        "        transfered_words.append(transfered_word)\n",
        "        counter = new_counter\n",
        "        normalized_word = normalizer.normalize(transfered_word)\n",
        "        lemmatized_word = lemmatizer.lemmatize(transfered_word)\n",
        "        if transfered_word in illegal_words or (transfered_word in abbreviation and abbreviation[transfered_word] in illegal_words) or normalized_word in illegal_words or lemmatized_word in illegal_words:\n",
        "            found_ill_words[transfered_word] = word\n",
        "        founded_words = find_similar_words_in_context(transfered_word)\n",
        "        for founded_word in founded_words:\n",
        "            normalized_founded_word = normalizer.normalize(founded_word)\n",
        "            lemmatized_founded_word = lemmatizer.lemmatize(founded_word)\n",
        "            if founded_word in illegal_words or (founded_word in abbreviation and abbreviation[founded_word] in illegal_words) or normalized_founded_word in illegal_words or lemmatized_founded_word in illegal_words:\n",
        "                found_ill_words[transfered_word] = word\n",
        "    \n",
        "    print_illegal_words(input, found_ill_words)\n",
        "\n",
        "    if status == 1: #ignoring space\n",
        "        found_spaced_ill_words = find_words_ignoring_spaces(splitted_input, transfered_words, indices)\n",
        "        print_illegal_words_space(found_spaced_ill_words)"
      ],
      "metadata": {
        "id": "Yo1NT9VtKMw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output"
      ],
      "metadata": {
        "id": "D0raf3V_QnpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &تف...ن۸# گ را فروختم\"\n",
        "illegal_words = ['تفنگ']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xomxby44WpQ",
        "outputId": "cd2fa957-18a2-4670-c759-b0dd09aad8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "With white space between each word: \n",
            "\n",
            "word: \"تفنگ\"\n",
            "span: (4, 16)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST\n",
        "-----\n"
      ],
      "metadata": {
        "id": "FLvIcIawPYd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test without spaces in each word"
      ],
      "metadata": {
        "id": "6XSI4TmO12eV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1:"
      ],
      "metadata": {
        "id": "7u9UzCTrRcZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &تف...ن۸گ# را فروختم\"\n",
        "illegal_words = ['تفنگ']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E578tIONdy8",
        "outputId": "998a21e3-db20-4554-8550-de2d9d0e24c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"تفنگ\"\n",
            "span: (4, 14)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 2:"
      ],
      "metadata": {
        "id": "Ho2koAZWRfxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"با ق*aاbشق و fچ^!نگ4ال غذا خوردم.\"\n",
        "illegal_words = ['قاشق', 'چنگال']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP6_wGRAP25r",
        "outputId": "3ec0388f-f4da-4f74-f88f-88feffa52d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"قاشق\"\n",
            "span: (3, 10)\n",
            "\n",
            "word: \"چنگال\"\n",
            "span: (13, 22)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with space in each word"
      ],
      "metadata": {
        "id": "p_YrAirA177p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1:"
      ],
      "metadata": {
        "id": "3dS3ONtl3V61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &تف...ن۸# گ را به اح* مد تو سنگ7%#3!8..اپور   فروختم\"\n",
        "illegal_words = ['سنگاپور', 'تفنگ', 'احمد']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLSyrTyp1-6s",
        "outputId": "0288ea04-1f2b-4bd9-df83-10f300e5dfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"سنگاپور\"\n",
            "span: (32, 47)\n",
            "\n",
            "\n",
            "With white space between each word: \n",
            "\n",
            "word: \"تفنگ\"\n",
            "span: (4, 16)\n",
            "\n",
            "word: \"احمد\"\n",
            "span: (22, 29)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"من دیروز به ا8یت۳۲ ا ل یا رفتم و با رض 49ا دیدار کردم\"\n",
        "illegal_words = ['ایتالیا', 'رضا']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9jeycqMxuJ9",
        "outputId": "a2e4d830-e54f-4790-8bf8-b67b91a1135b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "With white space between each word: \n",
            "\n",
            "word: \"ایتالیا\"\n",
            "span: (12, 26)\n",
            "\n",
            "word: \"رضا\"\n",
            "span: (36, 43)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with normalizer"
      ],
      "metadata": {
        "id": "NdzHNf1xD0U7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &دیــ...#گران را فروختم\"\n",
        "illegal_words = ['دیگران']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ROWloIUD27k",
        "outputId": "ad740a75-6bb2-40a1-b322-e8e7ddc3d9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"دیــگران\"\n",
            "span: (4, 17)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with lemmatizer"
      ],
      "metadata": {
        "id": "e7pBKYDLKeGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &گ..ل.ه#ا را فروختم\"\n",
        "illegal_words = ['گل']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clbdq_x5KhRH",
        "outputId": "32a078ae-da4f-466f-f850-39e828de95ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"گلها\"\n",
            "span: (4, 13)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with abbreviation in text"
      ],
      "metadata": {
        "id": "Eu_c3nE1vhQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &آ...ج#ا آپ فروختم\"\n",
        "illegal_words = ['ارتش جمهوری اسلامی ایران']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A3tmwROvlo-",
        "outputId": "b1c537b8-fa93-4b49-d3de-5ac80617838a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"آجا\"\n",
            "span: (4, 12)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &آ...ج#ا آ..پ# فروختم\"\n",
        "illegal_words = ['آسان پرداخت', 'ارتش جمهوری اسلامی ایران']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADaVT-dbSXX5",
        "outputId": "e1c6fb42-7fda-410d-e146-0be090eb0531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"آجا\"\n",
            "span: (4, 12)\n",
            "\n",
            "word: \"آپ\"\n",
            "span: (13, 18)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test with context"
      ],
      "metadata": {
        "id": "r2DTU0aPbULk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"این &س...ل#ام را فروختم\"\n",
        "illegal_words = ['درود']\n",
        "run(input, illegal_words)"
      ],
      "metadata": {
        "id": "HLl_vfnDbWzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e50ce6-d0a8-45df-a6dd-afec1d937d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without any space between each word: \n",
            "\n",
            "word: \"سلام\"\n",
            "span: (4, 13)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}